{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec_LiuJing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_89yP2qzEui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0ff8020-57df-4537-a4fa-ce538c8474b2"
      },
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow\n",
        "!pip install plot_keras_history\n",
        "!pip install seaborn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.43.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.23.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Collecting plot_keras_history\n",
            "  Downloading plot_keras_history-1.1.30.tar.gz (8.6 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from plot_keras_history) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from plot_keras_history) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from plot_keras_history) (1.4.1)\n",
            "Collecting sanitize_ml_labels>=1.0.28\n",
            "  Downloading sanitize_ml_labels-1.0.29.tar.gz (7.4 kB)\n",
            "Collecting compress_json\n",
            "  Downloading compress_json-1.0.4.tar.gz (4.7 kB)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->plot_keras_history) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->plot_keras_history) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->plot_keras_history) (2018.9)\n",
            "Building wheels for collected packages: plot-keras-history, sanitize-ml-labels, compress-json\n",
            "  Building wheel for plot-keras-history (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for plot-keras-history: filename=plot_keras_history-1.1.30-py3-none-any.whl size=8794 sha256=f100a5b943915da2434b11e99a58ebb7250a9c66352532d5744e1ab8302402a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/60/47/8c5aa37c06be5e97879ec467bc2e6a30b315d95f662c63a503\n",
            "  Building wheel for sanitize-ml-labels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sanitize-ml-labels: filename=sanitize_ml_labels-1.0.29-py3-none-any.whl size=7878 sha256=361f42e1548cdf97ab7d96e44dbb5aee9a6f74f8b7403c9e6b422d0d3608018d\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/f5/71/d1c459da10abec864a1979b449edbf37d4a82ab3e38a3625a8\n",
            "  Building wheel for compress-json (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compress-json: filename=compress_json-1.0.4-py3-none-any.whl size=4588 sha256=1b4cf6dab5f6313312accfc7a0f0cf43bcf6cf4790dbfa3ab20f1c994a44b10a\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/ef/1e/5d403c5632b0462471a8d26049d0c138134d0255ec60ce4c14\n",
            "Successfully built plot-keras-history sanitize-ml-labels compress-json\n",
            "Installing collected packages: compress-json, sanitize-ml-labels, plot-keras-history\n",
            "Successfully installed compress-json-1.0.4 plot-keras-history-1.1.30 sanitize-ml-labels-1.0.29\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.1.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtQk7TU5KqtU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29941488-77fd-43a2-ddb1-f30909511691"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeiZ_Cqvzoqe"
      },
      "source": [
        "\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda, Reshape\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers import dot\n",
        "from tensorflow.keras.activations import relu\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.sequence import skipgrams\n",
        "import gensim\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZazoZFH0BZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "299ed99c-16a5-42b0-8206-62b9b07d5635"
      },
      "source": [
        "# using nltk tokenizer.  \n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co69rL7jzrh3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2efba994-f62f-403c-e3fc-eba73dae5914"
      },
      "source": [
        "#Data Preparation \n",
        "\n",
        "AlotOftext = \"\"\"Language users never choose words randomly, and language is essentially\n",
        "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
        "posits randomness. Hence, when we look at linguistic phenomena in corpora, \n",
        "the null hypothesis will never be true. Moreover, where there is enough\n",
        "data, we shall (almost) always be able to establish that it is not true. In\n",
        "corpus studies, we frequently do have enough data, so the fact that a relation \n",
        "between two phenomena is demonstrably non-random, does not support the inference \n",
        "that it is not arbitrary. We present experimental evidence\n",
        "of how arbitrary associations between word frequencies and corpora are\n",
        "systematically non-random. We review literature in which hypothesis testing \n",
        "has been used, and show how it has often led to unhelpful or misleading results.\"\"\".lower()\n",
        "\n",
        "\n",
        "\n",
        "#Tokenize text\n",
        "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(AlotOftext)]\n",
        "\n",
        "#Create Vocab as a Dictionary\n",
        "vocab = Dictionary(tokenized_text)\n",
        "print(dict(vocab.items()))\n",
        "\n",
        "print(vocab.token2id['corpora'])\n",
        "print(vocab[2])\n",
        "sent0 = tokenized_text[0]\n",
        "print(vocab.doc2idx(sent0))\n",
        "\n",
        "vocab.add_documents([['PAD']])\n",
        "dict(vocab.items())\n",
        "print(vocab.token2id['PAD'])\n",
        "\n",
        "corpusByWordID = list()\n",
        "for sent in  tokenized_text:\n",
        "    corpusByWordID.append(vocab.doc2idx(sent))\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embed_size = 100\n",
        "hidden_dim=100\n",
        "window_size = 2 # context window size\n",
        "\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(vocab.items())[:10])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: ',', 1: '.', 2: 'and', 3: 'choose', 4: 'essentially', 5: 'is', 6: 'language', 7: 'never', 8: 'non-random', 9: 'randomly', 10: 'users', 11: 'words', 12: 'a', 13: 'hypothesis', 14: 'null', 15: 'posits', 16: 'randomness', 17: 'statistical', 18: 'testing', 19: 'uses', 20: 'which', 21: 'at', 22: 'be', 23: 'corpora', 24: 'hence', 25: 'in', 26: 'linguistic', 27: 'look', 28: 'phenomena', 29: 'the', 30: 'true', 31: 'we', 32: 'when', 33: 'will', 34: '(', 35: ')', 36: 'able', 37: 'almost', 38: 'always', 39: 'data', 40: 'enough', 41: 'establish', 42: 'it', 43: 'moreover', 44: 'not', 45: 'shall', 46: 'that', 47: 'there', 48: 'to', 49: 'where', 50: 'arbitrary', 51: 'between', 52: 'corpus', 53: 'demonstrably', 54: 'do', 55: 'does', 56: 'fact', 57: 'frequently', 58: 'have', 59: 'inference', 60: 'relation', 61: 'so', 62: 'studies', 63: 'support', 64: 'two', 65: 'are', 66: 'associations', 67: 'evidence', 68: 'experimental', 69: 'frequencies', 70: 'how', 71: 'of', 72: 'present', 73: 'systematically', 74: 'word', 75: 'been', 76: 'has', 77: 'led', 78: 'literature', 79: 'misleading', 80: 'often', 81: 'or', 82: 'results', 83: 'review', 84: 'show', 85: 'unhelpful', 86: 'used'}\n",
            "23\n",
            "and\n",
            "[6, 10, 7, 3, 11, 9, 0, 2, 6, 5, 4, 8, 1]\n",
            "87\n",
            "Vocabulary Size: 88\n",
            "Vocabulary Sample: [(0, ','), (1, '.'), (2, 'and'), (3, 'choose'), (4, 'essentially'), (5, 'is'), (6, 'language'), (7, 'never'), (8, 'non-random'), (9, 'randomly')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBxqCqAL1BCo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb3e938-8ac4-4f19-8253-3da0f9cc7398"
      },
      "source": [
        "# Create CBOW Training data\n",
        "def generate_cbow_context_word_pairs(corpusByID, window_size, vocab_size):\n",
        "    context_length = window_size*2\n",
        "    X=[]\n",
        "    Y=[]\n",
        "    for sent in corpusByID:\n",
        "        sentence_length = len(sent)\n",
        "        for index, word in enumerate(sent):\n",
        "            context_words = []\n",
        "            label_word   = []            \n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1\n",
        "            \n",
        "            context_words.append([sent[i] \n",
        "                                 for i in range(start, end) \n",
        "                                 if 0 <= i < sentence_length \n",
        "                                 and i != index])\n",
        "            label_word.append(word)\n",
        "            if start<0:\n",
        "                x = sequence.pad_sequences(context_words, maxlen=context_length,padding='pre',value=vocab.token2id['PAD'])\n",
        "                y = np_utils.to_categorical(label_word, vocab_size)\n",
        "                X.append(x)\n",
        "                Y.append(y)\n",
        "                continue\n",
        "            if end>=sentence_length:\n",
        "                x = sequence.pad_sequences(context_words, maxlen=context_length,padding='post',value=vocab.token2id['PAD'])\n",
        "                y = np_utils.to_categorical(label_word, vocab_size)\n",
        "                X.append(x)\n",
        "                Y.append(y)\n",
        "                continue\n",
        "            else:\n",
        "                X.append(sequence.pad_sequences(context_words, maxlen=context_length))\n",
        "                y = np_utils.to_categorical(label_word, vocab_size)\n",
        "                Y.append(y)\n",
        "                continue\n",
        "           \n",
        "    return X,Y\n",
        "            \n",
        "# Test this out for some samples\n",
        "\n",
        "\n",
        "X,Y = generate_cbow_context_word_pairs(corpusByWordID, window_size, vocab_size) \n",
        "   \n",
        "for x, y in zip(X,Y):\n",
        "    print('Context (X):', [vocab[w] for w in x[0]], '-> Target (Y):', vocab[np.argwhere(y[0])[0][0]])\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context (X): ['PAD', 'PAD', 'users', 'never'] -> Target (Y): language\n",
            "Context (X): ['PAD', 'language', 'never', 'choose'] -> Target (Y): users\n",
            "Context (X): ['language', 'users', 'choose', 'words'] -> Target (Y): never\n",
            "Context (X): ['users', 'never', 'words', 'randomly'] -> Target (Y): choose\n",
            "Context (X): ['never', 'choose', 'randomly', ','] -> Target (Y): words\n",
            "Context (X): ['choose', 'words', ',', 'and'] -> Target (Y): randomly\n",
            "Context (X): ['words', 'randomly', 'and', 'language'] -> Target (Y): ,\n",
            "Context (X): ['randomly', ',', 'language', 'is'] -> Target (Y): and\n",
            "Context (X): [',', 'and', 'is', 'essentially'] -> Target (Y): language\n",
            "Context (X): ['and', 'language', 'essentially', 'non-random'] -> Target (Y): is\n",
            "Context (X): ['language', 'is', 'non-random', '.'] -> Target (Y): essentially\n",
            "Context (X): ['is', 'essentially', '.', 'PAD'] -> Target (Y): non-random\n",
            "Context (X): ['essentially', 'non-random', 'PAD', 'PAD'] -> Target (Y): .\n",
            "Context (X): ['PAD', 'PAD', 'hypothesis', 'testing'] -> Target (Y): statistical\n",
            "Context (X): ['PAD', 'statistical', 'testing', 'uses'] -> Target (Y): hypothesis\n",
            "Context (X): ['statistical', 'hypothesis', 'uses', 'a'] -> Target (Y): testing\n",
            "Context (X): ['hypothesis', 'testing', 'a', 'null'] -> Target (Y): uses\n",
            "Context (X): ['testing', 'uses', 'null', 'hypothesis'] -> Target (Y): a\n",
            "Context (X): ['uses', 'a', 'hypothesis', ','] -> Target (Y): null\n",
            "Context (X): ['a', 'null', ',', 'which'] -> Target (Y): hypothesis\n",
            "Context (X): ['null', 'hypothesis', 'which', 'posits'] -> Target (Y): ,\n",
            "Context (X): ['hypothesis', ',', 'posits', 'randomness'] -> Target (Y): which\n",
            "Context (X): [',', 'which', 'randomness', '.'] -> Target (Y): posits\n",
            "Context (X): ['which', 'posits', '.', 'PAD'] -> Target (Y): randomness\n",
            "Context (X): ['posits', 'randomness', 'PAD', 'PAD'] -> Target (Y): .\n",
            "Context (X): ['PAD', 'PAD', ',', 'when'] -> Target (Y): hence\n",
            "Context (X): ['PAD', 'hence', 'when', 'we'] -> Target (Y): ,\n",
            "Context (X): ['hence', ',', 'we', 'look'] -> Target (Y): when\n",
            "Context (X): [',', 'when', 'look', 'at'] -> Target (Y): we\n",
            "Context (X): ['when', 'we', 'at', 'linguistic'] -> Target (Y): look\n",
            "Context (X): ['we', 'look', 'linguistic', 'phenomena'] -> Target (Y): at\n",
            "Context (X): ['look', 'at', 'phenomena', 'in'] -> Target (Y): linguistic\n",
            "Context (X): ['at', 'linguistic', 'in', 'corpora'] -> Target (Y): phenomena\n",
            "Context (X): ['linguistic', 'phenomena', 'corpora', ','] -> Target (Y): in\n",
            "Context (X): ['phenomena', 'in', ',', 'the'] -> Target (Y): corpora\n",
            "Context (X): ['in', 'corpora', 'the', 'null'] -> Target (Y): ,\n",
            "Context (X): ['corpora', ',', 'null', 'hypothesis'] -> Target (Y): the\n",
            "Context (X): [',', 'the', 'hypothesis', 'will'] -> Target (Y): null\n",
            "Context (X): ['the', 'null', 'will', 'never'] -> Target (Y): hypothesis\n",
            "Context (X): ['null', 'hypothesis', 'never', 'be'] -> Target (Y): will\n",
            "Context (X): ['hypothesis', 'will', 'be', 'true'] -> Target (Y): never\n",
            "Context (X): ['will', 'never', 'true', '.'] -> Target (Y): be\n",
            "Context (X): ['never', 'be', '.', 'PAD'] -> Target (Y): true\n",
            "Context (X): ['be', 'true', 'PAD', 'PAD'] -> Target (Y): .\n",
            "Context (X): ['PAD', 'PAD', ',', 'where'] -> Target (Y): moreover\n",
            "Context (X): ['PAD', 'moreover', 'where', 'there'] -> Target (Y): ,\n",
            "Context (X): ['moreover', ',', 'there', 'is'] -> Target (Y): where\n",
            "Context (X): [',', 'where', 'is', 'enough'] -> Target (Y): there\n",
            "Context (X): ['where', 'there', 'enough', 'data'] -> Target (Y): is\n",
            "Context (X): ['there', 'is', 'data', ','] -> Target (Y): enough\n",
            "Context (X): ['is', 'enough', ',', 'we'] -> Target (Y): data\n",
            "Context (X): ['enough', 'data', 'we', 'shall'] -> Target (Y): ,\n",
            "Context (X): ['data', ',', 'shall', '('] -> Target (Y): we\n",
            "Context (X): [',', 'we', '(', 'almost'] -> Target (Y): shall\n",
            "Context (X): ['we', 'shall', 'almost', ')'] -> Target (Y): (\n",
            "Context (X): ['shall', '(', ')', 'always'] -> Target (Y): almost\n",
            "Context (X): ['(', 'almost', 'always', 'be'] -> Target (Y): )\n",
            "Context (X): ['almost', ')', 'be', 'able'] -> Target (Y): always\n",
            "Context (X): [')', 'always', 'able', 'to'] -> Target (Y): be\n",
            "Context (X): ['always', 'be', 'to', 'establish'] -> Target (Y): able\n",
            "Context (X): ['be', 'able', 'establish', 'that'] -> Target (Y): to\n",
            "Context (X): ['able', 'to', 'that', 'it'] -> Target (Y): establish\n",
            "Context (X): ['to', 'establish', 'it', 'is'] -> Target (Y): that\n",
            "Context (X): ['establish', 'that', 'is', 'not'] -> Target (Y): it\n",
            "Context (X): ['that', 'it', 'not', 'true'] -> Target (Y): is\n",
            "Context (X): ['it', 'is', 'true', '.'] -> Target (Y): not\n",
            "Context (X): ['is', 'not', '.', 'PAD'] -> Target (Y): true\n",
            "Context (X): ['not', 'true', 'PAD', 'PAD'] -> Target (Y): .\n",
            "Context (X): ['PAD', 'PAD', 'corpus', 'studies'] -> Target (Y): in\n",
            "Context (X): ['PAD', 'in', 'studies', ','] -> Target (Y): corpus\n",
            "Context (X): ['in', 'corpus', ',', 'we'] -> Target (Y): studies\n",
            "Context (X): ['corpus', 'studies', 'we', 'frequently'] -> Target (Y): ,\n",
            "Context (X): ['studies', ',', 'frequently', 'do'] -> Target (Y): we\n",
            "Context (X): [',', 'we', 'do', 'have'] -> Target (Y): frequently\n",
            "Context (X): ['we', 'frequently', 'have', 'enough'] -> Target (Y): do\n",
            "Context (X): ['frequently', 'do', 'enough', 'data'] -> Target (Y): have\n",
            "Context (X): ['do', 'have', 'data', ','] -> Target (Y): enough\n",
            "Context (X): ['have', 'enough', ',', 'so'] -> Target (Y): data\n",
            "Context (X): ['enough', 'data', 'so', 'the'] -> Target (Y): ,\n",
            "Context (X): ['data', ',', 'the', 'fact'] -> Target (Y): so\n",
            "Context (X): [',', 'so', 'fact', 'that'] -> Target (Y): the\n",
            "Context (X): ['so', 'the', 'that', 'a'] -> Target (Y): fact\n",
            "Context (X): ['the', 'fact', 'a', 'relation'] -> Target (Y): that\n",
            "Context (X): ['fact', 'that', 'relation', 'between'] -> Target (Y): a\n",
            "Context (X): ['that', 'a', 'between', 'two'] -> Target (Y): relation\n",
            "Context (X): ['a', 'relation', 'two', 'phenomena'] -> Target (Y): between\n",
            "Context (X): ['relation', 'between', 'phenomena', 'is'] -> Target (Y): two\n",
            "Context (X): ['between', 'two', 'is', 'demonstrably'] -> Target (Y): phenomena\n",
            "Context (X): ['two', 'phenomena', 'demonstrably', 'non-random'] -> Target (Y): is\n",
            "Context (X): ['phenomena', 'is', 'non-random', ','] -> Target (Y): demonstrably\n",
            "Context (X): ['is', 'demonstrably', ',', 'does'] -> Target (Y): non-random\n",
            "Context (X): ['demonstrably', 'non-random', 'does', 'not'] -> Target (Y): ,\n",
            "Context (X): ['non-random', ',', 'not', 'support'] -> Target (Y): does\n",
            "Context (X): [',', 'does', 'support', 'the'] -> Target (Y): not\n",
            "Context (X): ['does', 'not', 'the', 'inference'] -> Target (Y): support\n",
            "Context (X): ['not', 'support', 'inference', 'that'] -> Target (Y): the\n",
            "Context (X): ['support', 'the', 'that', 'it'] -> Target (Y): inference\n",
            "Context (X): ['the', 'inference', 'it', 'is'] -> Target (Y): that\n",
            "Context (X): ['inference', 'that', 'is', 'not'] -> Target (Y): it\n",
            "Context (X): ['that', 'it', 'not', 'arbitrary'] -> Target (Y): is\n",
            "Context (X): ['it', 'is', 'arbitrary', '.'] -> Target (Y): not\n",
            "Context (X): ['is', 'not', '.', 'PAD'] -> Target (Y): arbitrary\n",
            "Context (X): ['not', 'arbitrary', 'PAD', 'PAD'] -> Target (Y): .\n",
            "Context (X): ['PAD', 'PAD', 'present', 'experimental'] -> Target (Y): we\n",
            "Context (X): ['PAD', 'we', 'experimental', 'evidence'] -> Target (Y): present\n",
            "Context (X): ['we', 'present', 'evidence', 'of'] -> Target (Y): experimental\n",
            "Context (X): ['present', 'experimental', 'of', 'how'] -> Target (Y): evidence\n",
            "Context (X): ['experimental', 'evidence', 'how', 'arbitrary'] -> Target (Y): of\n",
            "Context (X): ['evidence', 'of', 'arbitrary', 'associations'] -> Target (Y): how\n",
            "Context (X): ['of', 'how', 'associations', 'between'] -> Target (Y): arbitrary\n",
            "Context (X): ['how', 'arbitrary', 'between', 'word'] -> Target (Y): associations\n",
            "Context (X): ['arbitrary', 'associations', 'word', 'frequencies'] -> Target (Y): between\n",
            "Context (X): ['associations', 'between', 'frequencies', 'and'] -> Target (Y): word\n",
            "Context (X): ['between', 'word', 'and', 'corpora'] -> Target (Y): frequencies\n",
            "Context (X): ['word', 'frequencies', 'corpora', 'are'] -> Target (Y): and\n",
            "Context (X): ['frequencies', 'and', 'are', 'systematically'] -> Target (Y): corpora\n",
            "Context (X): ['and', 'corpora', 'systematically', 'non-random'] -> Target (Y): are\n",
            "Context (X): ['corpora', 'are', 'non-random', '.'] -> Target (Y): systematically\n",
            "Context (X): ['are', 'systematically', '.', 'PAD'] -> Target (Y): non-random\n",
            "Context (X): ['systematically', 'non-random', 'PAD', 'PAD'] -> Target (Y): .\n",
            "Context (X): ['PAD', 'PAD', 'review', 'literature'] -> Target (Y): we\n",
            "Context (X): ['PAD', 'we', 'literature', 'in'] -> Target (Y): review\n",
            "Context (X): ['we', 'review', 'in', 'which'] -> Target (Y): literature\n",
            "Context (X): ['review', 'literature', 'which', 'hypothesis'] -> Target (Y): in\n",
            "Context (X): ['literature', 'in', 'hypothesis', 'testing'] -> Target (Y): which\n",
            "Context (X): ['in', 'which', 'testing', 'has'] -> Target (Y): hypothesis\n",
            "Context (X): ['which', 'hypothesis', 'has', 'been'] -> Target (Y): testing\n",
            "Context (X): ['hypothesis', 'testing', 'been', 'used'] -> Target (Y): has\n",
            "Context (X): ['testing', 'has', 'used', ','] -> Target (Y): been\n",
            "Context (X): ['has', 'been', ',', 'and'] -> Target (Y): used\n",
            "Context (X): ['been', 'used', 'and', 'show'] -> Target (Y): ,\n",
            "Context (X): ['used', ',', 'show', 'how'] -> Target (Y): and\n",
            "Context (X): [',', 'and', 'how', 'it'] -> Target (Y): show\n",
            "Context (X): ['and', 'show', 'it', 'has'] -> Target (Y): how\n",
            "Context (X): ['show', 'how', 'has', 'often'] -> Target (Y): it\n",
            "Context (X): ['how', 'it', 'often', 'led'] -> Target (Y): has\n",
            "Context (X): ['it', 'has', 'led', 'to'] -> Target (Y): often\n",
            "Context (X): ['has', 'often', 'to', 'unhelpful'] -> Target (Y): led\n",
            "Context (X): ['often', 'led', 'unhelpful', 'or'] -> Target (Y): to\n",
            "Context (X): ['led', 'to', 'or', 'misleading'] -> Target (Y): unhelpful\n",
            "Context (X): ['to', 'unhelpful', 'misleading', 'results'] -> Target (Y): or\n",
            "Context (X): ['unhelpful', 'or', 'results', '.'] -> Target (Y): misleading\n",
            "Context (X): ['or', 'misleading', '.', 'PAD'] -> Target (Y): results\n",
            "Context (X): ['misleading', 'results', 'PAD', 'PAD'] -> Target (Y): .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH_iy4Lh1JUa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de67de9-cf89-446e-bf2e-6574694b20d2"
      },
      "source": [
        "cbow = Sequential()\n",
        "\n",
        "cbow.add(Embedding(input_dim=88, output_dim=100, input_length=4)) #N:100 V:23\n",
        "cbow.add(Lambda(lambda x: relu(K.mean(x, axis=1)), output_shape=(1,100)))\n",
        "\n",
        "cbow.add(Dense(88, activation='sigmoid'))\n",
        "cbow.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
        "cbow.summary()\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 4, 100)            8800      \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 100)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 88)                8888      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17,688\n",
            "Trainable params: 17,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuVqi7WGDxNe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33d96918-34f4-4e35-b988-75050abece29"
      },
      "source": [
        "#Train the model\n",
        "\n",
        "for epoch in range(100):\n",
        "    loss = 0.\n",
        "    for x, y in zip(X,Y):\n",
        "        loss += cbow.train_on_batch(x, y)\n",
        "    print(epoch, loss)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 644.529953956604\n",
            "1 642.6628098487854\n",
            "2 640.8561697006226\n",
            "3 639.1088786125183\n",
            "4 637.4187512397766\n",
            "5 635.7886962890625\n",
            "6 634.2181217670441\n",
            "7 632.70383644104\n",
            "8 631.2472808361053\n",
            "9 629.8468196392059\n",
            "10 628.5019631385803\n",
            "11 627.2107443809509\n",
            "12 625.9719932079315\n",
            "13 624.7825667858124\n",
            "14 623.640787601471\n",
            "15 622.5446374416351\n",
            "16 621.490226984024\n",
            "17 620.4775929450989\n",
            "18 619.5040559768677\n",
            "19 618.5655689239502\n",
            "20 617.6607971191406\n",
            "21 616.7875819206238\n",
            "22 615.941009759903\n",
            "23 615.1200044155121\n",
            "24 614.3197145462036\n",
            "25 613.5392379760742\n",
            "26 612.7749700546265\n",
            "27 612.0258460044861\n",
            "28 611.2883203029633\n",
            "29 610.5604948997498\n",
            "30 609.8407635688782\n",
            "31 609.126074552536\n",
            "32 608.4147922992706\n",
            "33 607.7050426006317\n",
            "34 606.9944343566895\n",
            "35 606.2834105491638\n",
            "36 605.5687637329102\n",
            "37 604.849258184433\n",
            "38 604.1248989105225\n",
            "39 603.3927493095398\n",
            "40 602.6533381938934\n",
            "41 601.9049561023712\n",
            "42 601.1480689048767\n",
            "43 600.3802502155304\n",
            "44 599.6040115356445\n",
            "45 598.8185274600983\n",
            "46 598.0249547958374\n",
            "47 597.2244116067886\n",
            "48 596.4170742034912\n",
            "49 595.603346824646\n",
            "50 594.7847635746002\n",
            "51 593.9635363817215\n",
            "52 593.1399109363556\n",
            "53 592.3163516521454\n",
            "54 591.4953047037125\n",
            "55 590.6774159669876\n",
            "56 589.8638772964478\n",
            "57 589.055764913559\n",
            "58 588.2535730600357\n",
            "59 587.459165096283\n",
            "60 586.671445608139\n",
            "61 585.8928699493408\n",
            "62 585.1229926347733\n",
            "63 584.3604491949081\n",
            "64 583.6048668622971\n",
            "65 582.8567804098129\n",
            "66 582.1153892278671\n",
            "67 581.3781179189682\n",
            "68 580.6474615931511\n",
            "69 579.9183750152588\n",
            "70 579.1938334107399\n",
            "71 578.4717383980751\n",
            "72 577.7497903704643\n",
            "73 577.0321410894394\n",
            "74 576.3145353198051\n",
            "75 575.5953011512756\n",
            "76 574.8772486448288\n",
            "77 574.156265437603\n",
            "78 573.4361003041267\n",
            "79 572.7119901776314\n",
            "80 571.9868203997612\n",
            "81 571.2569296360016\n",
            "82 570.5264979600906\n",
            "83 569.7914927601814\n",
            "84 569.0528047084808\n",
            "85 568.3104842305183\n",
            "86 567.5632602572441\n",
            "87 566.8111300468445\n",
            "88 566.0543162822723\n",
            "89 565.2926714420319\n",
            "90 564.5247902274132\n",
            "91 563.749859213829\n",
            "92 562.9679256081581\n",
            "93 562.1813817620277\n",
            "94 561.3886285424232\n",
            "95 560.5866522192955\n",
            "96 559.7785484194756\n",
            "97 558.9624935984612\n",
            "98 558.1387701034546\n",
            "99 557.3077135682106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxPVT_G-1RYz"
      },
      "source": [
        "## Save the wordvectors\n",
        "f = open('/content/drive/My Drive/TPML/day1/Cbow_vectors.txt' ,'w')\n",
        "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
        "vectors = cbow.get_weights()[0]\n",
        "for key in vocab:\n",
        "    str_vec = ' '.join(map(str, list(vectors[key, :])))\n",
        "    f.write('{} {}\\n'.format(vocab[key], str_vec))\n",
        "f.close()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1H3zTpE1Uwr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f42cd3-3f50-48a5-c0df-cbd0701ba592"
      },
      "source": [
        "## Load the vectors back and validate\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/My Drive/TPML/day1/Cbow_vectors.txt', binary=False)\n",
        "\n",
        "w2v.most_similar(positive=['language'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('words', 0.46368470788002014),\n",
              " ('randomly', 0.3626438081264496),\n",
              " ('non-random', 0.3214108943939209),\n",
              " ('which', 0.2801418602466583),\n",
              " ('the', 0.2698863744735718),\n",
              " ('.', 0.2316035032272339),\n",
              " ('hypothesis', 0.2199399769306183),\n",
              " ('choose', 0.21757295727729797),\n",
              " ('users', 0.20526540279388428),\n",
              " ('posits', 0.2004464864730835)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4z88oUR1Yk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f44781d2-17f5-46d0-f1ef-a97ffafa9b15"
      },
      "source": [
        "#Create Skipgram Training data \n",
        "\n",
        "# generate skip-grams with both positive and negative examples\n",
        "skip_grams = [skipgrams(sent, vocabulary_size=vocab_size, window_size=2) for sent in corpusByWordID]\n",
        "\n",
        "# view sample skip-grams\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "for i in range(10):\n",
        "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "        vocab[pairs[i][0]], pairs[i][0],           \n",
        "        vocab[pairs[i][1]], pairs[i][1], \n",
        "        labels[i]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(users (10), evidence (67)) -> 0\n",
            "(randomly (9), there (47)) -> 0\n",
            "(essentially (4), is (5)) -> 1\n",
            "(choose (3), linguistic (26)) -> 0\n",
            "(randomly (9), choose (3)) -> 1\n",
            "(language (6), frequently (57)) -> 0\n",
            "(users (10), never (7)) -> 1\n",
            "(words (11), never (7)) -> 1\n",
            "(words (11), randomly (9)) -> 1\n",
            "(language (6), word (74)) -> 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqD40Iq11fpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a46b52-7adf-4990-b44b-9765f679468d"
      },
      "source": [
        "#define the skip-gram model\n",
        "\n",
        "input_word = Input((1,))\n",
        "input_context_word = Input((1,))\n",
        "\n",
        "word_embedding    = Embedding(input_dim=88, output_dim=100,input_length=1,name='word_embedding')\n",
        "context_embedding = Embedding(input_dim=88, output_dim=100,input_length=1,name='conotext_embedding')\n",
        "\n",
        "word_embedding = word_embedding(input_word)\n",
        "word_embedding_layer = Reshape((100, 1))(word_embedding)\n",
        "\n",
        "context_embedding = context_embedding(input_context_word)\n",
        "context_embedding_layer = Reshape((100, 1))(context_embedding)\n",
        "\n",
        "# now perform the dot product operation  \n",
        "dot_product = dot([word_embedding_layer, context_embedding_layer], axes=1)\n",
        "dot_product = Reshape((1,))(dot_product)\n",
        "\n",
        "# add the sigmoid output layer\n",
        "outputLayer = Dense(1, activation='sigmoid')(dot_product)\n",
        "\n",
        "model = Model(inputs=[input_word, input_context_word], outputs=outputLayer)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# view model summary\n",
        "print(model.summary())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)          [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " input_14 (InputLayer)          [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " word_embedding (Embedding)     (None, 1, 100)       8800        ['input_13[0][0]']               \n",
            "                                                                                                  \n",
            " conotext_embedding (Embedding)  (None, 1, 100)      8800        ['input_14[0][0]']               \n",
            "                                                                                                  \n",
            " reshape_12 (Reshape)           (None, 100, 1)       0           ['word_embedding[0][0]']         \n",
            "                                                                                                  \n",
            " reshape_13 (Reshape)           (None, 100, 1)       0           ['conotext_embedding[0][0]']     \n",
            "                                                                                                  \n",
            " dot_4 (Dot)                    (None, 1, 1)         0           ['reshape_12[0][0]',             \n",
            "                                                                  'reshape_13[0][0]']             \n",
            "                                                                                                  \n",
            " reshape_14 (Reshape)           (None, 1)            0           ['dot_4[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 1)            2           ['reshape_14[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 17,602\n",
            "Trainable params: 17,602\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVpBqKfo1iSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d79f459-5f40-42c8-ae4a-63d8f4a5d8ce"
      },
      "source": [
        "#train the model\n",
        "\n",
        "for epoch in range(1, 100):\n",
        "    loss = 0\n",
        "    for i, elem in enumerate(skip_grams):\n",
        "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "        labels = np.array(elem[1], dtype='int32')\n",
        "        X = [pair_first_elem, pair_second_elem]\n",
        "        Y = labels\n",
        "        if i % 10000 == 0:\n",
        "            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
        "        loss += model.train_on_batch(X,Y)  \n",
        "\n",
        "    print('Epoch:', epoch, 'Loss:', loss)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 1 Loss: 0.5551827773451805\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 2 Loss: 0.5512690395116806\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 3 Loss: 0.5475252717733383\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 4 Loss: 0.5439421236515045\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 5 Loss: 0.5405109599232674\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 6 Loss: 0.5372236967086792\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 7 Loss: 0.534072682261467\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 8 Loss: 0.5310508050024509\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 9 Loss: 0.5281513631343842\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 10 Loss: 0.5253680236637592\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 11 Loss: 0.5226949714124203\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 12 Loss: 0.5201265588402748\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 13 Loss: 0.5176576413214207\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 14 Loss: 0.5152832493185997\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 15 Loss: 0.5129987895488739\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 16 Loss: 0.5107999481260777\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 17 Loss: 0.5086825713515282\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 18 Loss: 0.5066427662968636\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 19 Loss: 0.5046769455075264\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 20 Loss: 0.5027816481888294\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 21 Loss: 0.5009536035358906\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 22 Loss: 0.4991897642612457\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 23 Loss: 0.497487161308527\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 24 Loss: 0.49584315344691277\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 25 Loss: 0.49425509572029114\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 26 Loss: 0.4927205257117748\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 27 Loss: 0.4912371523678303\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 28 Loss: 0.48980267718434334\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 29 Loss: 0.48841507732868195\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 30 Loss: 0.4870723783969879\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 31 Loss: 0.48577267676591873\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 32 Loss: 0.48451416939496994\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 33 Loss: 0.4832951948046684\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 34 Loss: 0.48211411014199257\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 35 Loss: 0.4809694327414036\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 36 Loss: 0.4798596613109112\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 37 Loss: 0.4787834547460079\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 38 Loss: 0.4777394086122513\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 39 Loss: 0.47672636434435844\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 40 Loss: 0.47574305906891823\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 41 Loss: 0.4747883081436157\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 42 Loss: 0.47386113926768303\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 43 Loss: 0.4729604981839657\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 44 Loss: 0.472085315734148\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 45 Loss: 0.4712347239255905\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 46 Loss: 0.47040778771042824\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 47 Loss: 0.46960366144776344\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 48 Loss: 0.4688214883208275\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 49 Loss: 0.46806052327156067\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 50 Loss: 0.4673199951648712\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 51 Loss: 0.4665991887450218\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 52 Loss: 0.46589742973446846\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 53 Loss: 0.4652140140533447\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 54 Loss: 0.46454836800694466\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 55 Loss: 0.4638998545706272\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 56 Loss: 0.4632679261267185\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 57 Loss: 0.46265194565057755\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 58 Loss: 0.46205148845911026\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 59 Loss: 0.461465984582901\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 60 Loss: 0.46089494228363037\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 61 Loss: 0.46033788844943047\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 62 Loss: 0.45979439467191696\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 63 Loss: 0.45926403254270554\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 64 Loss: 0.45874636992812157\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 65 Loss: 0.45824097469449043\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 66 Loss: 0.45774752274155617\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 67 Loss: 0.45726563408970833\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 68 Loss: 0.4567948989570141\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 69 Loss: 0.45633503422141075\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 70 Loss: 0.45588570833206177\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 71 Loss: 0.4554465413093567\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 72 Loss: 0.4550173059105873\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 73 Loss: 0.45459767431020737\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 74 Loss: 0.4541873373091221\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 75 Loss: 0.45378607138991356\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 76 Loss: 0.4533935561776161\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 77 Loss: 0.45300960913300514\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 78 Loss: 0.45263392850756645\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 79 Loss: 0.45226630941033363\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 80 Loss: 0.4519065171480179\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 81 Loss: 0.4515542984008789\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 82 Loss: 0.45120951160788536\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 83 Loss: 0.45087189599871635\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 84 Loss: 0.45054126158356667\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 85 Loss: 0.45021743327379227\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 86 Loss: 0.44990021735429764\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 87 Loss: 0.44958943501114845\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 88 Loss: 0.4492849223315716\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 89 Loss: 0.4489865079522133\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 90 Loss: 0.44869400188326836\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 91 Loss: 0.448407307267189\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 92 Loss: 0.4481262229382992\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 93 Loss: 0.44785063713788986\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 94 Loss: 0.44758040457963943\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 95 Loss: 0.447315338999033\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 96 Loss: 0.447055384516716\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 97 Loss: 0.4468003623187542\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 98 Loss: 0.4465501382946968\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 99 Loss: 0.4463046081364155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjhkbLsp1k0Y"
      },
      "source": [
        "#get the embeding matrix\n",
        "weights = model.get_weights()\n",
        "## Save the wordvectors\n",
        "f = open('/content/drive/My Drive/TPML/day1/skipgram_vectors.txt' ,'w')\n",
        "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
        "vectors = model.get_weights()[0]\n",
        "for key in vocab:\n",
        "    str_vec = ' '.join(map(str, list(vectors[key, :])))\n",
        "    f.write('{} {}\\n'.format(vocab[key], str_vec))\n",
        "f.close()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4rUPCJC1mvc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0801bfe8-ea1c-418e-8913-f7acd7277a02"
      },
      "source": [
        "## Load the vectors back and validate\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/My Drive/TPML/day1/skipgram_vectors.txt', binary=False)\n",
        "w2v.most_similar(positive=['the'])\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('look', 0.3385516405105591),\n",
              " ('a', 0.32431089878082275),\n",
              " ('uses', 0.3201105296611786),\n",
              " ('posits', 0.2946311831474304),\n",
              " ('testing', 0.28061509132385254),\n",
              " ('relation', 0.27493759989738464),\n",
              " ('it', 0.26850587129592896),\n",
              " ('which', 0.26102298498153687),\n",
              " ('does', 0.2528742551803589),\n",
              " ('review', 0.2403210699558258)]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs8ng_Zf1o08"
      },
      "source": [
        "#Excerise: \n",
        "#modeify the skipegram_model to share the same embeding layer between word and context\n",
        "#Discussion: which is better? Why?  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modeify the skipegram_share_model\n",
        "input_s_word = Input((1,))\n",
        "input_s_context_word = Input((1,))\n",
        "\n",
        "share_embedding   = Embedding(input_dim=88, output_dim=100,input_length=1,name='share_embedding')\n",
        "\n",
        "word_embedding = share_embedding(input_s_word)\n",
        "word_embedding_layer = Reshape((100, 1))(word_embedding)\n",
        "\n",
        "context_embedding = share_embedding(input_s_context_word)\n",
        "context_embedding_layer = Reshape((100, 1))(context_embedding)\n",
        "\n",
        "# now perform the dot product operation  \n",
        "dot_product = dot([word_embedding_layer, context_embedding_layer], axes=1)\n",
        "dot_product = Reshape((1,))(dot_product)\n",
        "\n",
        "# add the sigmoid output layer\n",
        "outputLayer = Dense(1, activation='sigmoid')(dot_product)\n",
        "\n",
        "share_model = Model(inputs=[input_s_word, input_s_context_word], outputs=outputLayer)\n",
        "share_model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# view model summary\n",
        "print(share_model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyFUSERDscAC",
        "outputId": "edce9a46-2f79-4bf3-f3a7-5879b645bc60"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_15 (InputLayer)          [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " input_16 (InputLayer)          [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " share_embedding (Embedding)    (None, 1, 100)       8800        ['input_15[0][0]',               \n",
            "                                                                  'input_16[0][0]']               \n",
            "                                                                                                  \n",
            " reshape_15 (Reshape)           (None, 100, 1)       0           ['share_embedding[0][0]']        \n",
            "                                                                                                  \n",
            " reshape_16 (Reshape)           (None, 100, 1)       0           ['share_embedding[1][0]']        \n",
            "                                                                                                  \n",
            " dot_5 (Dot)                    (None, 1, 1)         0           ['reshape_15[0][0]',             \n",
            "                                                                  'reshape_16[0][0]']             \n",
            "                                                                                                  \n",
            " reshape_17 (Reshape)           (None, 1)            0           ['dot_5[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 1)            2           ['reshape_17[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,802\n",
            "Trainable params: 8,802\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train the model\n",
        "\n",
        "for epoch in range(1, 100):\n",
        "    loss = 0\n",
        "    for i, elem in enumerate(skip_grams):\n",
        "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "        labels = np.array(elem[1], dtype='int32')\n",
        "        X = [pair_first_elem, pair_second_elem]\n",
        "        Y = labels\n",
        "        if i % 10000 == 0:\n",
        "            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
        "        loss += share_model.train_on_batch(X,Y)  \n",
        "\n",
        "    print('Epoch:', epoch, 'Loss:', loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLGXqSLv55IG",
        "outputId": "aeab3f6a-a8ea-4ec7-f33b-e54c511181c2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 1 Loss: 0.793669693171978\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 2 Loss: 0.7903274074196815\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 3 Loss: 0.787090927362442\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 4 Loss: 0.783955529332161\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 5 Loss: 0.7809169143438339\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 6 Loss: 0.7779709696769714\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 7 Loss: 0.7751136943697929\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 8 Loss: 0.7723414823412895\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 9 Loss: 0.7696507349610329\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 10 Loss: 0.7670380920171738\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 11 Loss: 0.7645004242658615\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 12 Loss: 0.7620346322655678\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 13 Loss: 0.7596378549933434\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 14 Loss: 0.757307380437851\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 15 Loss: 0.7550405785441399\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 16 Loss: 0.7528349906206131\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 17 Loss: 0.7506882548332214\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 18 Loss: 0.7485980838537216\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 19 Loss: 0.7465623989701271\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 20 Loss: 0.7445790842175484\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 21 Loss: 0.7426462322473526\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 22 Loss: 0.7407619804143906\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 23 Loss: 0.7389246076345444\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 24 Loss: 0.7371323108673096\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 25 Loss: 0.7353835627436638\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 26 Loss: 0.7336767837405205\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 27 Loss: 0.7320104762911797\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 28 Loss: 0.7303832024335861\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 29 Loss: 0.7287937551736832\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 30 Loss: 0.727240689098835\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 31 Loss: 0.7257228195667267\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 32 Loss: 0.7242390364408493\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 33 Loss: 0.7227880582213402\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 34 Loss: 0.7213689386844635\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 35 Loss: 0.7199805304408073\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 36 Loss: 0.7186219096183777\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 37 Loss: 0.7172921225428581\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 38 Loss: 0.7159902304410934\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 39 Loss: 0.7147153094410896\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 40 Loss: 0.7134666368365288\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 41 Loss: 0.7122432291507721\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 42 Loss: 0.7110444381833076\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 43 Loss: 0.709869422018528\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 44 Loss: 0.7087175771594048\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 45 Loss: 0.7075881287455559\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 46 Loss: 0.7064804509282112\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 47 Loss: 0.705393835902214\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 48 Loss: 0.7043277248740196\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 49 Loss: 0.703281506896019\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 50 Loss: 0.702254630625248\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 51 Loss: 0.7012465372681618\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 52 Loss: 0.7002566307783127\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 53 Loss: 0.6992845088243484\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 54 Loss: 0.6983296498656273\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 55 Loss: 0.6973915547132492\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 56 Loss: 0.696469783782959\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 57 Loss: 0.6955638602375984\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 58 Loss: 0.6946734115481377\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 59 Loss: 0.6937979832291603\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 60 Loss: 0.6929371580481529\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 61 Loss: 0.6920906454324722\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 62 Loss: 0.6912579908967018\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 63 Loss: 0.6904388889670372\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 64 Loss: 0.6896329447627068\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 65 Loss: 0.6888398453593254\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 66 Loss: 0.6880592852830887\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 67 Loss: 0.6872909516096115\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 68 Loss: 0.6865344867110252\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 69 Loss: 0.6857896894216537\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 70 Loss: 0.6850562170147896\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 71 Loss: 0.6843337938189507\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 72 Loss: 0.6836221739649773\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 73 Loss: 0.6829210594296455\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 74 Loss: 0.6822302266955376\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 75 Loss: 0.6815495118498802\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 76 Loss: 0.6808785796165466\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 77 Loss: 0.6802172288298607\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 78 Loss: 0.6795652508735657\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 79 Loss: 0.6789224222302437\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 80 Loss: 0.6782885268330574\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 81 Loss: 0.6776634082198143\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 82 Loss: 0.6770468205213547\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 83 Loss: 0.6764385998249054\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 84 Loss: 0.6758385300636292\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 85 Loss: 0.6752465292811394\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 86 Loss: 0.674662321805954\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 87 Loss: 0.6740857884287834\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 88 Loss: 0.6735167279839516\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 89 Loss: 0.6729550361633301\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 90 Loss: 0.6724005192518234\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 91 Loss: 0.6718530654907227\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 92 Loss: 0.6713124886155128\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 93 Loss: 0.6707786396145821\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 94 Loss: 0.6702514067292213\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 95 Loss: 0.66973065584898\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 96 Loss: 0.6692162305116653\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 97 Loss: 0.668708048760891\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 98 Loss: 0.6682059839367867\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 99 Loss: 0.6677099093794823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get the embeding matrix\n",
        "weights = share_model.get_weights()\n",
        "## Save the wordvectors\n",
        "f = open('/content/drive/My Drive/TPML/day1/skipgram_vectors.txt' ,'w')\n",
        "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
        "vectors = share_model.get_weights()[0]\n",
        "for key in vocab:\n",
        "    str_vec = ' '.join(map(str, list(vectors[key, :])))\n",
        "    f.write('{} {}\\n'.format(vocab[key], str_vec))\n",
        "f.close()"
      ],
      "metadata": {
        "id": "vROiOWLA6ITN"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the vectors back and validate\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/My Drive/TPML/day1/skipgram_vectors.txt', binary=False)\n",
        "w2v.most_similar(positive=['the'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEnFhDQn6kdF",
        "outputId": "73d3e9cb-97c0-4661-f67f-4dcf3b482994"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fact', 0.6873430013656616),\n",
              " ('so', 0.6473961472511292),\n",
              " ('support', 0.5860694050788879),\n",
              " ('null', 0.562027096748352),\n",
              " ('corpora', 0.5151245594024658),\n",
              " ('do', 0.3742136061191559),\n",
              " ('not', 0.3738800287246704),\n",
              " ('have', 0.3358570337295532),\n",
              " ('that', 0.303596556186676),\n",
              " ('frequently', 0.30346593260765076)]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Discussion: which is better? Why? \n",
        "The skipegarm_model shared the same embedding layer is better.\n",
        "Because compared with the result of the separated one,it has higher similarity rank."
      ],
      "metadata": {
        "id": "jxIrzcMzGAw1"
      }
    }
  ]
}